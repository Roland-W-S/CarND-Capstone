{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traddif light classifier learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2918"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data list\n",
    "PATH_IMG_LIST_CSV=\"./data_sim.csv\"\n",
    "PATH_IMG_DIR=\".\"\n",
    "#PATH_IMG_LIST_CSV=\"../ros/src/tl_detector/training_data/data.csv\"\n",
    "#PATH_IMG_DIR=\"../ros/src/tl_detector\"\n",
    "img_list_csv=pd.read_csv(PATH_IMG_LIST_CSV,sep=',',header=None)\n",
    "cls_raw=img_list_csv[1]\n",
    "filenames=img_list_csv[0]\n",
    "data_num_raw=len(cls_raw)\n",
    "data_num_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "batch_size=64\n",
    "learning_rate=1e-3\n",
    "num_epoch=800\n",
    "resized_width=256\n",
    "resized_height=192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5836"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load images\n",
    "#resize 600,800->292,256\n",
    "\n",
    "data_num=data_num_raw*2\n",
    "data=np.zeros([data_num_raw*2,resized_width,resized_height,3])\n",
    "cls=np.zeros(data_num_raw*2)\n",
    "for i,fn in enumerate(filenames):\n",
    "    image_bgr = cv2.imread(PATH_IMG_DIR+\"/\"+fn)\n",
    "    #image=cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
    "    image=cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HLS)\n",
    "    resized_image=cv2.resize(image,(resized_height,resized_width))\n",
    "    data[i,:,:,:]=resized_image\n",
    "    #Data Augmentation\n",
    "    data[i+data_num_raw,:,:,:]=resized_image[::-1,:,:]\n",
    "    cls[i]=cls_raw[i]\n",
    "    cls[i+data_num_raw]=cls_raw[i]\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "\n",
    "#Shuffle data\n",
    "np.random.seed(0)\n",
    "shuffle_idx=np.random.permutation(cls.shape[0])\n",
    "\n",
    "#Split data indices\n",
    "train_num=int(data_num*0.6)\n",
    "test_num=int(data_num*0.2)\n",
    "val_num=data_num-train_num-test_num\n",
    "train_idx=shuffle_idx[:train_num]\n",
    "test_idx=shuffle_idx[train_num:train_num+test_num]\n",
    "val_idx=shuffle_idx[train_num+test_num:]\n",
    "\n",
    "#Save indices\n",
    "np.savetxt(\"dataset_train.csv\",train_idx)\n",
    "np.savetxt(\"dataset_test.csv\",test_idx)\n",
    "np.savetxt(\"dataset_val.csv\",val_idx)\n",
    "\n",
    "#Split data\n",
    "train_cls=cls[train_idx]\n",
    "train_data=data[train_idx]\n",
    "test_cls=cls[test_idx]\n",
    "test_data=data[test_idx]\n",
    "val_cls=cls[val_idx]\n",
    "val_data=data[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "\n",
    "#Input\n",
    "x_input_shape=(None,resized_width,resized_height,3)\n",
    "y_input_shape=(None,)\n",
    "x_input=tf.placeholder(tf.float32,shape=x_input_shape,name=\"x_input\")\n",
    "y_input=tf.placeholder(tf.int32,shape=y_input_shape,name=\"y_input\")\n",
    "y_input_onehot=tf.one_hot(y_input, depth=3, dtype=tf.float32)\n",
    "\n",
    "keep_prob=tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "\n",
    "#conv 1\n",
    "#192,256 -> 192,256 *32\n",
    "conv1_w=tf.Variable(tf.truncated_normal([3,3,3,32]))\n",
    "conv1_b=tf.Variable(tf.truncated_normal([32]))\n",
    "\n",
    "conv1_c=tf.nn.conv2d(x_input,conv1_w,strides=[1,1,1,1], padding='SAME')+conv1_b\n",
    "conv1_a=tf.nn.relu(conv1_c)\n",
    "\n",
    "#dropout 1\n",
    "drop1 = tf.nn.dropout(conv1_a, keep_prob)\n",
    "\n",
    "#pool 1\n",
    "#192,256 *32 -> 96,128 *32\n",
    "pool1=tf.nn.max_pool(drop1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "#conv 2\n",
    "#96,128*32 -> 96,128*64\n",
    "conv2_w=tf.Variable(tf.truncated_normal([3,3,32,64]))\n",
    "conv2_b=tf.Variable(tf.truncated_normal([64]))\n",
    "\n",
    "conv2_c=tf.nn.conv2d(pool1,conv2_w,strides=[1,1,1,1], padding='SAME')+conv2_b\n",
    "conv2_a=tf.nn.relu(conv2_c)\n",
    "\n",
    "#dropout 2\n",
    "drop2 = tf.nn.dropout(conv2_a, keep_prob)\n",
    "\n",
    "#pool 2\n",
    "#96,128*64 -> 48*64*64\n",
    "pool2=tf.nn.max_pool(drop2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#conv 3\n",
    "#48*64*64 -> 48*64*128\n",
    "conv3_w=tf.Variable(tf.truncated_normal([3,3,64,128]))\n",
    "conv3_b=tf.Variable(tf.truncated_normal([128]))\n",
    "\n",
    "conv3_c=tf.nn.conv2d(pool2,conv3_w,strides=[1,1,1,1], padding='SAME')+conv3_b\n",
    "conv3_a=tf.nn.relu(conv3_c)\n",
    "\n",
    "#pool 3\n",
    "#48*64*128 -> 24*32*128\n",
    "pool3=tf.nn.max_pool(conv3_a, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#fc\n",
    "#24*32*128 ->3\n",
    "fc1_w=tf.Variable(tf.truncated_normal([24*32*128,3]))\n",
    "fc1_b=tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "fc1_in=tf.reshape(pool3,[-1,24*32*128])\n",
    "fc1_c=tf.matmul(fc1_in,fc1_w)+fc1_b\n",
    "\n",
    "#Prediction, accuracy\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_input_onehot, logits=fc1_c))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "prediction=tf.argmax(fc1_c, 1,name=\"prediction\")\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(y_input_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32),name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(x,y,batch_size):\n",
    "    test_pred=np.zeros(0)\n",
    "    for i in range(int(x.shape[0]/batch_size)):\n",
    "        start_idx=i*batch_size\n",
    "        end_idx=min((i+1)*batch_size,data.shape[0]-1)\n",
    "        cur_x=x[start_idx:end_idx,:,:,:]\n",
    "        cur_y=y[start_idx:end_idx]\n",
    "        tmp=correct_prediction.eval(feed_dict={x_input: cur_x, y_input: cur_y, keep_prob: 1.0})\n",
    "        test_pred=np.concatenate([test_pred,tmp],axis=0)\n",
    "    test_accuracy=np.mean(test_pred)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train acc 0.375000  test acc 0.381944\n",
      "epoch 20, train acc 0.531250  test acc 0.588542\n",
      "epoch 40, train acc 0.687500  test acc 0.649306\n",
      "epoch 60, train acc 0.750000  test acc 0.694444\n",
      "epoch 80, train acc 0.765625  test acc 0.704861\n",
      "epoch 100, train acc 0.828125  test acc 0.724826\n",
      "epoch 120, train acc 0.828125  test acc 0.756076\n",
      "epoch 140, train acc 0.859375  test acc 0.769965\n",
      "epoch 160, train acc 0.859375  test acc 0.777778\n",
      "epoch 180, train acc 0.890625  test acc 0.801215\n",
      "epoch 200, train acc 0.921875  test acc 0.816840\n",
      "epoch 220, train acc 0.906250  test acc 0.818576\n",
      "epoch 240, train acc 0.906250  test acc 0.821181\n",
      "epoch 260, train acc 0.906250  test acc 0.821181\n",
      "epoch 280, train acc 0.906250  test acc 0.829861\n",
      "epoch 300, train acc 0.921875  test acc 0.838542\n",
      "epoch 320, train acc 0.921875  test acc 0.843750\n",
      "epoch 340, train acc 0.937500  test acc 0.848090\n",
      "epoch 360, train acc 0.953125  test acc 0.856771\n",
      "epoch 380, train acc 0.953125  test acc 0.855035\n",
      "epoch 400, train acc 0.921875  test acc 0.853299\n",
      "epoch 420, train acc 0.953125  test acc 0.866319\n",
      "epoch 440, train acc 0.968750  test acc 0.880208\n",
      "epoch 460, train acc 0.968750  test acc 0.880208\n",
      "epoch 480, train acc 0.968750  test acc 0.888021\n",
      "epoch 500, train acc 0.968750  test acc 0.888021\n",
      "epoch 520, train acc 0.953125  test acc 0.880208\n",
      "epoch 540, train acc 0.968750  test acc 0.886285\n",
      "epoch 560, train acc 0.968750  test acc 0.891493\n",
      "epoch 580, train acc 0.984375  test acc 0.898438\n",
      "epoch 600, train acc 0.968750  test acc 0.891493\n",
      "epoch 620, train acc 0.984375  test acc 0.897569\n",
      "epoch 640, train acc 0.984375  test acc 0.895833\n",
      "epoch 660, train acc 0.984375  test acc 0.901042\n",
      "epoch 680, train acc 0.984375  test acc 0.899306\n",
      "epoch 700, train acc 1.000000  test acc 0.907118\n",
      "epoch 720, train acc 1.000000  test acc 0.905382\n",
      "epoch 740, train acc 1.000000  test acc 0.907118\n",
      "epoch 760, train acc 1.000000  test acc 0.905382\n",
      "epoch 780, train acc 0.984375  test acc 0.903646\n",
      "validation acc 0.40625\n"
     ]
    }
   ],
   "source": [
    "#Model test\n",
    "tf.set_random_seed(0)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for j in range(num_epoch):\n",
    "        for i in range(int(train_data.shape[0]/batch_size)):\n",
    "            start_idx=i*batch_size\n",
    "            end_idx=min((i+1)*batch_size,data.shape[0]-1)\n",
    "            cur_x=train_data[start_idx:end_idx,:,:,:]\n",
    "            cur_y=train_cls[start_idx:end_idx]\n",
    "            train_step.run(feed_dict={x_input: cur_x, y_input: cur_y, keep_prob: 0.7})\n",
    "        if j % 20==0:\n",
    "            train_accuracy=accuracy.eval(feed_dict={x_input: cur_x, y_input: cur_y, keep_prob: 1.0})\n",
    "            #test_accuracy=accuracy.eval(feed_dict={x_input: test_data, y_input: test_cls, keep_prob: 1})\n",
    "            test_accuracy=eval_acc(test_data,test_cls,batch_size)\n",
    "            print('epoch %d, train acc %f  test acc %f' % (j, train_accuracy,test_accuracy))\n",
    "            #Save model\n",
    "            save_path = saver.save(sess, \"./tl_classifier.ckpt\")\n",
    "    valid_acc=eval_acc(test_data,val_cls,batch_size)\n",
    "    print('validation acc %g' % valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training accuracy 0.40625\n",
      "epoch 20, training accuracy 0.40625\n",
      "epoch 40, training accuracy 0.421875\n",
      "epoch 60, training accuracy 0.4375\n",
      "epoch 80, training accuracy 0.453125\n",
      "epoch 100, training accuracy 0.484375\n",
      "epoch 120, training accuracy 0.546875\n",
      "epoch 140, training accuracy 0.546875\n",
      "epoch 160, training accuracy 0.5625\n",
      "epoch 180, training accuracy 0.578125\n",
      "epoch 200, training accuracy 0.640625\n",
      "epoch 220, training accuracy 0.625\n",
      "epoch 240, training accuracy 0.640625\n",
      "epoch 260, training accuracy 0.640625\n",
      "epoch 280, training accuracy 0.65625\n",
      "epoch 300, training accuracy 0.6875\n",
      "epoch 320, training accuracy 0.734375\n",
      "epoch 340, training accuracy 0.703125\n",
      "epoch 360, training accuracy 0.78125\n",
      "epoch 380, training accuracy 0.765625\n",
      "epoch 400, training accuracy 0.796875\n",
      "epoch 420, training accuracy 0.765625\n",
      "epoch 440, training accuracy 0.796875\n",
      "epoch 460, training accuracy 0.859375\n",
      "epoch 480, training accuracy 0.84375\n",
      "epoch 500, training accuracy 0.890625\n",
      "epoch 520, training accuracy 0.84375\n",
      "epoch 540, training accuracy 0.90625\n",
      "epoch 560, training accuracy 0.90625\n",
      "epoch 580, training accuracy 0.90625\n",
      "epoch 600, training accuracy 0.953125\n",
      "epoch 620, training accuracy 0.90625\n",
      "epoch 640, training accuracy 0.9375\n",
      "epoch 660, training accuracy 0.953125\n",
      "epoch 680, training accuracy 0.90625\n",
      "epoch 700, training accuracy 0.953125\n",
      "epoch 720, training accuracy 0.921875\n",
      "epoch 740, training accuracy 0.9375\n",
      "epoch 760, training accuracy 0.984375\n",
      "epoch 780, training accuracy 0.984375\n",
      "validation acc 0.996528\n",
      "all data accuracy 0.997253\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "#Training final model (using all data)\n",
    "tf.set_random_seed(0)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    count=0\n",
    "    for j in range(num_epoch):\n",
    "        for i in range(int(data.shape[0]/batch_size)):\n",
    "            count+=1\n",
    "            start_idx=i*batch_size\n",
    "            end_idx=min((i+1)*batch_size,data.shape[0]-1)\n",
    "            cur_x=data[start_idx:end_idx,:,:,:]\n",
    "            cur_y=cls[start_idx:end_idx]\n",
    "            train_step.run(feed_dict={x_input: cur_x, y_input: cur_y, keep_prob: 0.7})\n",
    "        if j % 20==0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x_input: cur_x, y_input: cur_y, keep_prob: 1})\n",
    "            print('epoch %d, training accuracy %g' % (j, train_accuracy))\n",
    "\n",
    "    val_acc=eval_acc(val_data,val_cls,batch_size)\n",
    "    all_acc=eval_acc(data,cls,batch_size)\n",
    "    print('validation acc %g' % val_acc)\n",
    "    print('all data accuracy %g' % all_acc)\n",
    "    #Save model\n",
    "    save_path = saver.save(sess, \"./tl_classifier.ckpt\")\n",
    "    print('saved')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tl_classifier.ckpt\n",
      "all data accuracy 0.997253\n"
     ]
    }
   ],
   "source": [
    "#Loading test\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(\"tl_classifier.ckpt.meta\")\n",
    "    saver.restore(sess, \"./tl_classifier.ckpt\")\n",
    "    x_input=sess.graph.get_tensor_by_name(\"x_input:0\")\n",
    "    y_input=sess.graph.get_tensor_by_name(\"y_input:0\")\n",
    "    prediction=sess.graph.get_tensor_by_name(\"prediction:0\")\n",
    "    accuracy=sess.graph.get_tensor_by_name(\"accuracy:0\")\n",
    "    all_acc=eval_acc(data,cls,batch_size)\n",
    "    print('all data accuracy %g' % all_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
